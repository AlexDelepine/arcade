<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Validation Principles and Policy </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Validation Principles and Policy ">
      
      <link rel="icon" href="../../favicon.ico">
      <link rel="stylesheet" href="../../public/docfx.min.css">
      <link rel="stylesheet" href="../../public/main.css">
      <meta name="docfx:navrel" content="">
      <meta name="docfx:tocrel" content="">
      
      
      
      
      
  </head>

  <script type="module">
    import options from './../../public/main.js'
    import { init } from './../../public/docfx.min.js'
    init(options)
  </script>

  <script>
    const theme = localStorage.getItem('theme') || 'auto'
    document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
  </script>


  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../../index.html">
            <img id="logo" class="svg" src="../../logo.svg" alt="">
            
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" style="margin-top: -.65em; margin-left: -.8em" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="">
<h1 id="validation-principles-and-policy">Validation Principles and Policy</h1>

<ul>
<li>All code changes (e.g. new features and functionality, refactored code, impact of removed code, et cetera) must be testable in staging.</li>
<li>All code changes must be described in an issue, including the motivation for the change and links to corresponding pull requests.</li>
<li>All test results (including, but not limited to, unit, scenario, functional tests) should be viewable from the same location (e.g. all test results are currently being reported to Azure DevOps Test Results). See section <a href="#consistency-in-reporting">&quot;Consistency in Reporting&quot;</a> for how this is impelmented.</li>
<li>We should strive for 80% code coverage for our unit tests. This give us some leeway when we are dealing with generated code which we may not be able to unit test completely, or if we have some legacy code that cannot be easily unit tested and will need to be covered by functional or scenario tests instead. The goal is to make us think about all the possible code paths, possible corner/edge cases, and different types of data that can be passed into our methods that we will want to validate the outcome of. The code coverage reports can help us determine which code paths were missed in our unit tests so that we can meet (or exceed) the goal of 80% code coverage.</li>
<li>Efforts should be made to <a href="https://en.wikipedia.org/wiki/Shift-left_testing">&quot;shift left&quot;</a> for testing. Therefore:
<ul>
<li>Developers should validate their changes prior to creating a pull request. This may require testing locally on their dev machine or creating a virtual machine in Azure to test an artifact, et cetera.</li>
<li>Where possible, unit testing should be used. See the <a href="#unit-testing">Validation Process</a> for guidance for adding unit tests and creating new unit test projects that will hook into pull request and continuous integration job runs.</li>
<li>Scenario tests should be written to support testing customer use cases. These can be written as a part of the <a href="#post-deployment">post-deployment scenario tests</a>.</li>
<li>If there are known resources that are required for a deployment to be successful, <a href="#pre-deployment">pre-deployment checks</a> should be written.</li>
<li>Functionality that uses dependency injection (DI) requires tests to validate that the dependency injection works.</li>
</ul>
</li>
</ul>
<h1 id="validation-process-overview">Validation Process Overview</h1>
<p>Business Requirement: The process for running tests and reporting results uses a standard model across all services</p>
<h2 id="validation-and-deployment-workflow">Validation and Deployment Workflow</h2>
<p>Ideal developer workflow from dev environment to production:</p>
<ol>
<li>Developer is responsible for writing code for feature, unit/functional tests to cover code written for feature, and expanding on any scenario tests that changes or enhances the way the service works.</li>
<li>When the feature is completed, the developer should open a pull request that will validate their code. Upon passing validation (and gaining approval on their PR from their peers), the developer should merge into the main branch.</li>
<li><a href="#pre-deployment">Pre-deployment</a> checks on staging should occur to see if the services in staging are healthy for us to deploy to. If not, we should investigate and resolve the problems before we can deploy.</li>
<li>Code is built from main and deployed to staging.</li>
<li><a href="#post-deployment">Post-deployment</a> checks on staging should occur to ensure that the code we deployed is working as intended.</li>
<li>Similarly, we'll have pre-deployment checks on production prior to deploying our code to production.</li>
<li>Code is deployed to production.</li>
<li>Post-deployment validation:
<ol>
<li>Similar post-deployment checks on production, if necessary.</li>
<li>A <a href="#nightly">nightly</a> job, currently configured to run only on the staging environment, handles scenario tests that produce more load on our staging environment than we want to be running during regular business hours when staging is used for regular PR and CI jobs.</li>
</ol>
</li>
</ol>
<p><img src="Images/ValidationDeploymentWorkflow.svg" alt="Validation and Deployment Workflow"></p>
<h2 id="specific-validation-for-services-covered">Specific Validation for Services Covered</h2>
<p>The following links are to documents regarding specifics for validating these services. It includes links to pipelines and jobs, definitions of specific scenarios and things being tested for that service,</p>
<ul>
<li><a href="HelixValidation.html">Helix Services and API</a></li>
<li><a href="OSOBValidation.html">OS Onboarding</a></li>
<li><a href="https://github.com/dotnet/arcade/blob/main/Documentation/Validation/Overview.md">Arcade</a></li>
<li>Arcade Services</li>
</ul>
<h2 id="consistency-in-testing-services">Consistency in Testing Services</h2>
<p>All the test projects for all services should exist in the same solution as the service being tested. The following documentation is guidance for how to set up unit tests, scenario tests, and how to categorize tests.</p>
<h3 id="test-categorization">Test Categorization</h3>
<p>Categorization of our C# tests allow us to streamline the running of tests in our Azure DevOps pipelines. By organizing them this way, we can identify the category of tests that we want to run at a given time. For example, all the tests categorized with <code>PostDeployment</code> and <code>Staging</code> are configured up to run after a deployment to staging has occurred.</p>
<p>Adding a category to a test (or a whole test class or test project) in <a href="https://github.com/nunit/docs/wiki/Category-Attribute">NUnit</a> can be done as follows:</p>
<pre><code>[Category(&quot;PostDeployment&quot;, &quot;Staging&quot;)]
public void TestName...
</code></pre>
<p>This shows that the test called <code>TestName</code> has the categories of <code>PostDeployment</code> and <code>Staging</code>. If a test run task in Azure DevOps ran either of those categories, it would run that test. If it required both of those categories, it would also run.</p>
<p><em>Note: For <a href="http://www.brendanconnolly.net/organizing-tests-with-xunit-traits/">XUnit tests</a>, you'll need to use the <code>Trait</code> attribute instead of <code>Category</code>.</em></p>
<p><em>Note: We currently are supporting both XUnit and NUnit testing frameworks. However, we prefer to use XUnit where possible. NUnit was introduced only to support parallelization for our functional and scenario tests. If neither one of these testing frameworks works for your testing use case, start a discussion with the team to determine a solution. See <a href="https://github.com/dotnet/core-eng/pull/8568#discussion_r377821502">this discussion</a> for more information.</em></p>
<p>The following are categories that have been defined so far, but we are not limited to only using these categories. These categories are not mutually exclusive, so if there are tests that should be run in both Production and Staging, they can be categorized as such.</p>
<h4 id="pre-deployment">Pre-Deployment</h4>
<p>Criteria:</p>
<ul>
<li>Does the test validate the state of the environment being deployed to?</li>
<li>Does the test validate the existence of necessary resources for deployment?</li>
</ul>
<p>Examples of pre-deployment scenario tests:</p>
<ul>
<li>Can we connect to the Service Fabric cluster that we need to deploy to?</li>
<li>Do the secrets we use exist in the appropriate Key Vaults?</li>
<li>Do the Resource Groups required exist in Azure?</li>
</ul>
<p><em>Category Attribute Value</em>: <code>PreDeployment</code></p>
<h4 id="post-deployment">Post-Deployment</h4>
<p>Criteria:</p>
<ul>
<li>Does the test validate the state of the environment after deployment?</li>
<li>Does the test require other services in Azure (e.g. Service Bus, Scalesets) to run?</li>
<li>Do we not have the ability to run this test locally on a dev environment?</li>
</ul>
<p>Examples of post-deployment scenario tests:</p>
<ul>
<li>Testing endpoints in Helix API</li>
<li>Deployed service health check (e.g. are they running, are they returning expected results, et cetera)</li>
<li>Was the Helix database schema changed appropriately?</li>
</ul>
<p><em>Category Attribute Value</em>: <code>PostDeployment</code></p>
<h4 id="nightly">Nightly</h4>
<p>Criteria:</p>
<ul>
<li>Does the test do things that may disrupt normal work?</li>
<li>Do we want the test to run on a regular cadence instead of on-demand?</li>
</ul>
<p>Examples of nightly scenario tests:</p>
<ul>
<li>Service Fabric Chaos test</li>
<li>Load testing sending jobs to Helix queues</li>
</ul>
<p><em>Category Attribute Value</em>: <code>Nightly</code></p>
<h4 id="staging">Staging</h4>
<p>Criteria:</p>
<ul>
<li>Does the test validate functionality prior to being rolled out to production?</li>
<li>Does the test validate the state of the staging environment?</li>
</ul>
<p>Examples of staging scenario tests:</p>
<ul>
<li>Testing jobs sent to Helix to validate deployments to staging. We do this on staging only because if we were to run them on Production they would end up behind customer jobs, and customers would catch the bugs before our tests would.</li>
</ul>
<p><em>Category Attribute Value</em>: <code>Staging</code></p>
<h4 id="production">Production</h4>
<p>Criteria:</p>
<ul>
<li>Does the test monitor or test functionality after it has been deployed to production?</li>
<li>Does the test validate the state of the production environment?</li>
</ul>
<p>Examples of production scenario tests:</p>
<ul>
<li>Verifying the existence of required resource groups on the Production environment.</li>
<li>Tests that would monitor the health of our services and queues (not necessarily tied to a post-deployment scenario).</li>
</ul>
<p><em>Category Attribute Value</em>: <code>Production</code></p>
<h3 id="scenario-tests">Scenario Tests</h3>
<p>Scenario tests are defined as a test that encompasses a full functionality path (which is the scenario). There is an expected outcome for these tests based on the input provided. Usually, these tests will span multiple methods or even services. For example, we can create a test that creates a job, sends the work item to a specific queue, and verifies the job entry in our SQL database. This scenario would test multiple services and functionality in our ecosystem, such as the Helix API, Helix Controller, OS Onboarding, et cetera.</p>
<p>Scenario tests should encompass things like common functionality used by our customers, functionality that was able to reproduce a problem in our service (e.g. load test that was able to reproduce <a href="https://github.com/dotnet/core-eng/issues/7548">the issue</a> in AppInsights), et cetera. The tests should be categorized so that they can be run during specific pipelines. These tests are also useful for reproducing issues found by customers and building up a test suite to prevent future regressions, or ensuring that any refactoring that takes place will still provide the same expected result.</p>
<p>Scenario Test Criteria:</p>
<ul>
<li>The test will cover all the functionality hit during a specific customer scenario. (e.g. Customer sends a job to a specific Helix queue that does some specific work)</li>
<li>There is an expected result for the test.</li>
<li>The test can be repeated with the same expected outcome.</li>
<li>Other expected results can be validated (e.g. data sent to SQL or Kusto, state changes for other resources) along the way.</li>
</ul>
<p>Scenario test projects should be configured as follows:</p>
<ol>
<li><p>Create a folder called <code>Validation</code> within the solution for the service.</p>
</li>
<li><p>Create a C# test project for the type of the tests you want to add. Example:</p>
</li>
</ol>
<p><img src="Images/scenariotestprojects.png" alt="scenariotestprojects"></p>
<p>The project name should include the service being tested, that it is a test project, the environment that will be targeted, and when the tests will run. In the above screenshot, we have <code>Helix.Test.Staging.Nightly</code> which denotes that this test project will run nightly on staging for Helix tests. The <code>Helix.Test.Utilities</code> project is a helper project that does not contain tests.</p>
<ol start="3">
<li>Each test project should contain a <code>AssemblyInfo.cs</code> file that should contain the Category Attributes for the target environment and when the tests should run. For example, in NUnit:</li>
</ol>
<pre><code>[assembly: Category(&quot;Nightly&quot;)]
[assembly: Category(&quot;Staging&quot;)]
</code></pre>
<h3 id="unit-testing">Unit Testing</h3>
<p>Unlike scenario tests, <a href="https://docs.microsoft.com/en-us/dotnet/core/testing/#what-are-unit-tests">unit tests</a> should be targeted to a specific piece of functionality, preferably within a single method. An example of a unit test would be validating that a method that calculates a value does it correctly or that it handles bad input gracefully (or returns some kind of error or other expected response). Unit tests should not require additional services in order to verify the test. Services/resources/classes/modules/et cetera outside the scope of the test should be mocked out. These tests should also be able to be ran locally on any developer's environment. Being able to run the tests locally allows developers to validate that code before committing it to the repository, or have to rely on a pull request to validate their changes.</p>
<p>When developing unit tests, you may find it appropriate to mock out dependencies to support the test. Our preferred mocking framework is <a href="https://github.com/Moq/moq4/wiki/Quickstart">Moq</a> for our C# code. You might also find it necessary to use adapter/wrapper patterns to support mocking out dependencies (e.g. see <a href="https://dnceng.visualstudio.com/internal/_git/dotnet-helix-service?path=%2Fsrc%2FUtility%2FHelix.Utility.Logging%2FTelemetryClientProxy.cs">TelemetryClientProxy.cs</a> class created to support mocking the <code>TelemetryClient</code> class from <code>Microsoft.ApplicationInsights</code> library.)</p>
<p>(Dec 12, 2019) Note: These steps have been validated to work in the Helix Services and Arcade Services, and still need to be validated to work with OSOB.</p>
<p>To add a unit test project, and ensure that it is able to automatically run in the pipelines and have it's test results and code coverage collected, do the following steps:</p>
<ol>
<li>Create the new test project (using xUnit): Right click on the solution (or folder where you want to add the test project) -&gt; Add -&gt; New Project... -&gt; select xUnit Test Project (.NET Core)</li>
</ol>
<p><img src="Images/newxunitproject.png" alt="newxunitproject"></p>
<p>Alternatively, you can create a new project via the dotnet CLI with the command: <code>dotnet new xunit</code></p>
<p>Use the same name as the source project the tests will be for, but with &quot;.Tests&quot; at the end. Based on the Validation process diagram, this project should existing beside the project that it will be testing. This allows for better visibility of the test project in relation to the code it will be testing.</p>
<p><img src="Images/testproject.png" alt="testproject"></p>
<ol start="2">
<li>The following package references should be added to the .csproj:</li>
</ol>
<pre><code>  &lt;PackageReference Include=&quot;xunit&quot; /&gt;
  &lt;PackageReference Include=&quot;xunit.runner.visualstudio&quot; /&gt;
  &lt;PackageReference Include=&quot;Microsoft.NET.Test.Sdk&quot; /&gt;
  &lt;PackageReference Include=&quot;Microsoft.CodeCoverage&quot; /&gt;
</code></pre>
<ol start="3">
<li>Rename the namespace of the new test class with the rest of the tests in the solution. Example:
<code>Microsoft.Internal.Helix.Utility.Tests</code></li>
<li>Include the source project to be tested as a project reference: Right click on the test project -&gt; Add -&gt; Reference... -&gt; Select the checkbox for the source project. This is what it would look like in the .csproj file:</li>
</ol>
<pre><code>  &lt;ItemGroup&gt;
    &lt;ProjectReference Include=&quot;..\..\Utility\Helix.Utility.Common\Helix.Utility.Common.csproj&quot; /&gt;
  &lt;/ItemGroup&gt;
</code></pre>
<ol start="5">
<li>Ensure that the test project is targeting the correct Target Framework Moniker (TFM). This should be the consistent with the source project's TFM. Example:</li>
</ol>
<pre><code>  &lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;$(NetFrameworkFramework)&lt;/TargetFramework&gt;
  &lt;/PropertyGroup&gt;
</code></pre>
<ol start="6">
<li>Projects that use the .NET Core framework <a href="https://github.com/Microsoft/vstest/issues/800">require an additional property</a> to ensure the symbols required for code coverage are published. In each source project being tested (not the test project), add the following:</li>
</ol>
<pre><code>  &lt;PropertyGroup&gt;
    &lt;DebugType&gt;Full&lt;/DebugType&gt;
  &lt;/PropertyGroup&gt;
</code></pre>
<ol start="7">
<li>Excluding third party libraries from code coverage can be accomplished through <a href="https://docs.microsoft.com/en-us/visualstudio/test/customizing-code-coverage-analysis?view=vs-2019">adding a .runsettings file</a>. This file will need to be referenced in the stage that collects code coverage. The contents of the .runsettings file should look like this:</li>
</ol>
<pre><code>  &lt;RunSettings&gt;
    &lt;DataCollectionRunSettings&gt;
      &lt;DataCollectors&gt;
        &lt;DataCollector friendlyName=&quot;Code Coverage&quot; uri=&quot;datacollector://Microsoft/CodeCoverage/2.0&quot; assemblyQualifiedName=&quot;Microsoft.VisualStudio.Coverage.DynamicCoverageDataCollector, Microsoft.VisualStudio.TraceCollector, Version=11.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a&quot;&gt;
          &lt;Configuration&gt;
            &lt;CodeCoverage&gt;
              &lt;ModulePaths&gt;
                &lt;Exclude&gt;
                  &lt;ModulePath&gt;.*Moq.dll&lt;/ModulePath&gt;
                  &lt;ModulePath&gt;.*fluentassertions.dll&lt;/ModulePath&gt;
                  &lt;ModulePath&gt;.*microsoft.extensions.logging.applicationinsights.dll&lt;/ModulePath&gt;
                  &lt;ModulePath&gt;.*libgit2sharp.dll&lt;/ModulePath&gt;
                  &lt;ModulePath&gt;.*quartz.dll&lt;/ModulePath&gt;
                &lt;/Exclude&gt;
              &lt;/ModulePaths&gt;
            &lt;/CodeCoverage&gt;
          &lt;/Configuration&gt;
        &lt;/DataCollector&gt;
      &lt;/DataCollectors&gt;
    &lt;/DataCollectionRunSettings&gt;
  &lt;/RunSettings&gt;
</code></pre>
<h4 id="testing-barviz">Testing BARViz</h4>
<p>There is <a href="https://github.com/dotnet/arcade-services/tree/master/src/Maestro/maestro-angular">documentation in Arcade Services</a> regarding steps to test locally. Also, there is a task in Arcade Services pipeline that will run these tests via <code>npm test</code> during PRs/CI.</p>
<p>Notes:</p>
<ul>
<li>Currently, this process has only been verified to work with xUnit tests. Although we do have nUnit in our codebase, it was introduced to help with parallelization of our functional and scenario tests.</li>
<li>Why are the package references in step 2 needed?
<ul>
<li><strong>xunit</strong>: used for the xUnit test framework.</li>
<li><strong>xunit.runner.visualstudio</strong>: required for Visual Studio to run the xunit tests.</li>
<li><strong>Microsoft.NET.Test.Sdk</strong>: this package (specifically version 15.8.0+) is required so that the <code>dotnet test</code> command can collect code coverage during the PR and CI-merge-to-master pipeline runs.</li>
<li><strong>Microsoft.CodeCoverage</strong>: this package ensures that the CodeCoverage.exe tool will always be available in our builds, especially to help us reduce our dependency on Visual Studio Enterprise.</li>
</ul>
</li>
<li>If the repository is using <a href="https://github.com/dotnet/arcade">Arcade</a>, the unit test results will automatically be collected. If it's not, then the build pipeline will require a step to upload the test results.</li>
</ul>
<h2 id="consistency-in-reporting">Consistency in Reporting</h2>
<p>When our tests fail, we want to be able to see what failure(s) occurred so that we can remediate the issue(s). We want to have a standard way for the tests to report on the test results, and allow us to see the health of our builds and deployments based on the tests that have ran.</p>
<p>All results from all types of tests should be uploaded to Azure DevOps test results to have a single location to view our test results. Depending on the Azure DevOps task used, the test results may be uploaded to the Test Results automatically. Although it would be ideal for all our services to run tests the same way, it would be unreasonable to expect that because not all our services are engineered the same way (e.g. Arcade-Services uses Arcade's process to build and upload test results, versus Helix Services that makes use of the VSTest Azure DevOps task to run and upload test results). The expectation is that the test results for any test (whether it's unit or scenario) should show up in Azure DevOps Test Results.</p>
<p>If a project is not set up to run tests, here are some snippets to help incorporate that functionality into the project's Azure DevOps build/test/deploy pipelines:</p>
<h3 id="using-vstest-task">Using VSTest Task</h3>
<p>This is a snippet from the Helix Services <code>deploy-staging.yaml</code> for running the post-deployment scenario tests. The VSTest task will automatically upload the results from this task to Azure DevOps Test Results.</p>
<p>This task will run the tests in <code>Helix.Test.Staging.PostDeployment.dll</code> with the test categories of <code>Staging</code> and <code>PostDeployment</code></p>
<pre><code>- task: VsTest@2
  displayName: Post Deployment Scenario Tests
  ...
  inputs:
    testSelector: testAssemblies
    testAssemblyVer2: |
      **\Helix.Test.Staging.PostDeployment.dll
    searchFolder: '$(Pipeline.Workspace)/Deploy_Staging'
    testFiltercriteria: &gt; 
      TestCategory=Staging&amp;
      TestCategory=PostDeployment
    ...
    testRunTitle: Post Deployment Scenario Tests
</code></pre>
<h3 id="using-dotnetcorecli-task">Using DotNetCoreCLI Task</h3>
<p>This is a snippet from the Helix Services <code>test.yaml</code> for running the unit tests, and collecting code coverage reports. Tests results from here are not automatically uploaded, so it will require calling the <code>PublishTestResults</code> task as well.</p>
<p>This task will run any test that fits the pattern of <code>src/**/*.Tests.csproj</code>, and omits any test that is categorized for <code>PostDeployment</code>, <code>Nightly</code>, or <code>PreDeployment</code>, since we only want unit tests to run in this task.</p>
<pre><code>- task: DotNetCoreCLI@2
  displayName: Test C# (dotnet test)
  inputs:
    command: 'custom'
    projects: 'src/**/*.Tests.csproj'
    custom: 'test'
    arguments: &gt; 
      --configuration $(BuildConfiguration)
      --collect:&quot;Code Coverage&quot;
      --settings:$(Build.SourcesDirectory)/src/CodeCoverage.runsettings
      --filter &quot;TestCategory!=PostDeployment&amp;TestCategory!=Nightly&amp;TestCategory!=PreDeployment&quot;
      --logger trx
      --no-build
  condition: succeededOrFailed()

- task: PublishTestResults@2
  displayName: 'Publish Unit Test Results'
  inputs: 
    testResultsFormat: VSTest
    testResultsFiles: '**/*.trx'
    mergeTestResults: true
    searchFolder: $(system.defaultworkingdirectory)
    testRunTitle: Helix Unit Tests
</code></pre>
<h3 id="code-coverage-reports">Code Coverage Reports</h3>
<p>Code coverage reports should also be available for PR/CI pipelines and will report on how much code is covered via unit tests (if configured correctly per the guidance above). At the moment, the reports are generated for only C# unit tests, and uses a series of Azure DevOps tasks to complete. The following code snippets are how code coverage is currently set up for our services that employ it's use (as of Jan 10, 2020, Arcade Services and Helix Services).</p>
<p><em>Note: A fair amount of the following was created leveraging information from <a href="https://medium.com/swlh/generating-code-coverage-reports-in-dotnet-core-bb0c8cca66">this article</a>.</em></p>
<p>Code coverage files are collected using the DotNetCoreCLI task. There were some challenges with doing this, namely that we have to use the <code>custom</code> command versus <code>test</code> as the <code>test</code> command does not support code coverage collection. This task will also run and collect unit test results.</p>
<pre><code>- task: DotNetCoreCLI@2
  displayName: Test C# (dotnet test)
  inputs:
    command: 'custom'
    projects: 'src/**/*.Tests.csproj'
    custom: 'test'
    arguments: &gt; 
      --configuration $(BuildConfiguration)
      --collect:&quot;Code Coverage&quot;
      --settings:$(Build.SourcesDirectory)/src/CodeCoverage.runsettings
      --filter &quot;TestCategory!=PostDeployment&amp;TestCategory!=Nightly&amp;TestCategory!=PreDeployment&quot;
      --logger trx
      --no-build
  condition: succeededOrFailed()
</code></pre>
<p>Next, we need to convert the <code>.coverage</code> file that was generated in the previous step to XML. The <code>convert-codecoveragetoxml.ps1</code> Powershell script finds the <code>.coverage</code> file, and uses <code>CodeCoverage.exe</code> to convert to XML. (<em>Note: This Powershell script file is a custom script that is added to each /eng/ folder in repositories that do this work.</em>)</p>
<pre><code>- task: Powershell@2
  inputs: 
    targetType: 'filePath'
    filePath: eng\convert-codecoveragetoxml.ps1
    arguments: -Path &quot;$(system.defaultworkingdirectory)&quot; -NugetPackagesPath &quot;$(Build.SourcesDirectory)\packages&quot;
  displayName: Convert Code Coverage to XML (powershell)
</code></pre>
<p>To generate the fancy report, we use the ReportGenerator Azure DevOps plug-in.</p>
<pre><code>- task: Palmmedia.reportgenerator.reportgenerator-build-release-task.reportgenerator@4
  displayName: ReportGenerator
  inputs:
    reports: '$(system.defaultworkingdirectory)\codecoverage.coveragexml'
    targetdir: '$(Build.SourcesDirectory)\CodeCoverage'
    reporttypes: 'HtmlInline_AzurePipelines;Cobertura'
    sourcedirs: '$(Build.SourcesDirectory)'
</code></pre>
<p>Lastly, we need to upload both the code coverage report that was generated from the previous step and the unit test results from the above test run.</p>
<pre><code>- task: PublishCodeCoverageResults@1
  displayName: 'Publish Code Coverage'
  inputs:
    codeCoverageTool: Cobertura
    summaryFileLocation: '$(Build.SourcesDirectory)\CodeCoverage\Cobertura.xml'
    reportDirectory: '$(Build.SourcesDirectory)\CodeCoverage'
    pathToSources: '$(Build.SourcesDirectory)'
    publishRunAttachments: true

- task: PublishTestResults@2
  displayName: 'Publish Unit Test Results'
  inputs: 
    testResultsFormat: VSTest
    testResultsFiles: '**/*.trx'
    mergeTestResults: true
    searchFolder: $(system.defaultworkingdirectory)
    testRunTitle: Helix Unit Tests
</code></pre>
<h4 id="code-coverage-reports-locally">Code Coverage Reports Locally</h4>
<p>To get a report of code coverage locally, you can run the Code Coverage Analyzer from Visual Studio Enterprise. Follow the instructions <a href="https://docs.microsoft.com/en-us/visualstudio/test/using-code-coverage-to-determine-how-much-code-is-being-tested?view=vs-2019">here</a> for how to run and view code coverage results.</p>
<!-- Begin Generated Content: Doc Feedback -->
<p><sub>Was this helpful? <a href="https://helix.dot.net/f/p/5?p=Documentation%5CValidation%5CREADME.md"><img src="https://helix.dot.net/f/ip/5?p=Documentation%5CValidation%5CREADME.md" alt="Yes"></a> <a href="https://helix.dot.net/f/n/5?p=Documentation%5CValidation%5CREADME.md"><img src="https://helix.dot.net/f/in" alt="No"></a></sub></p>
<!-- End Generated Content-->
</article>

        <div class="contribution d-print-none">
        </div>

        <div class="next-article d-print-none border-top" id="nextArticle"></div>
        
      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>


    <footer class="border-top">
      <div class="container-xxl">
        <div class="flex-fill">
          <span>Made with <a href="https://dotnet.github.io/docfx">docfx</a></span>
        </div>
      </div>
    </footer>
  </body>
</html>